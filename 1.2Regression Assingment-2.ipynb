{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05913ea7-b2af-42c9-a4ae-f4afd175ef52",
   "metadata": {},
   "source": [
    "### Q1. Explain the concept of R-squared in linear regression models. How is it calculated, and what does it represent ?\n",
    "##### Answer:-\n",
    "R-squared (Coefficient of Determination) measures how well the independent variables explain the variability of the dependent variable in a regression model. It is calculated as:\n",
    "\n",
    "ùëÖ2=1‚àí SSres/SStot\n",
    "\n",
    "Interpretation: R-squared represents the proportion of the variance in the dependent variable that is predictable from the independent variables. Values range from 0 to 1, with 1 indicating that the model explains all the variability, and 0 indicating that it explains none."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c381c30-cd64-43a6-b111-e9fce3ef19fc",
   "metadata": {},
   "source": [
    "### Q2. Define adjusted R-squared and explain how it differs from the regular R-squared.\n",
    "##### Answer:-\n",
    "Adjusted R-squared adjusts the R-squared value based on the number of predictors in the model. It penalizes the addition of irrelevant predictors. The formula is:\n",
    "\n",
    "Adjusted¬†R^2= 1- (1-R^2/n-p-1)* (n-1)\n",
    "Where: n = Number of observations.\n",
    "  p = Number of predictors.\n",
    "\n",
    "Difference: Unlike R-squared, which always increases with the addition of predictors, adjusted R-squared can decrease if the new predictors do not improve the model sufficiently. It provides a more accurate measure of model fit, especially for multiple regression models. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f3738c-828c-4d86-a381-65fcfc907eee",
   "metadata": {},
   "source": [
    "### Q3. When is it more appropriate to use adjusted R-squared?\n",
    "##### Answer:-\n",
    "Adjusted R-squared is more appropriate when comparing models with different numbers of predictors. It is useful in:\n",
    "\n",
    "Model Selection: To avoid overfitting by accounting for the number of predictors.\n",
    "\n",
    "Model Comparison: To compare models with different numbers of predictors more accurately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0ab609-5382-48cf-a278-913cf59860a4",
   "metadata": {},
   "source": [
    "### Q4. What are RMSE, MSE, and MAE in the context of regression analysis? How are these metrics calculated, and what do they represent?\n",
    "##### Answer:-\n",
    "\n",
    "Root Mean Squared Error (RMSE): Measures the square root of the average squared differences between predicted and actual values. It is sensitive to outliers.\n",
    "\n",
    "RMSE= ‚àö n/1 i=1‚àën(yi‚àíy^i)2\n",
    "\n",
    "Mean Squared Error (MSE): Measures the average of the squared differences between predicted and actual values. It is also sensitive to outliers.\n",
    "\n",
    "MSE= n/1 i=1‚àën(yi‚àíy^i)2\n",
    "\n",
    "Mean Absolute Error (MAE): Measures the average of the absolute differences between predicted and actual values. It is less sensitive to outliers than RMSE and MSE.\n",
    "\n",
    "MAE= n/1 i=1‚àën|yi‚àíy^i|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940c18ca-a525-4b65-ae49-9fa033e99974",
   "metadata": {},
   "source": [
    "### Q5. Discuss the advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in regression analysis.\n",
    "##### Answer:- \n",
    "RMSE:\n",
    "\n",
    "Advantages: Sensitive to large errors, which makes it useful if large errors are particularly undesirable.\n",
    "Disadvantages: Sensitive to outliers and the scale of the data, which can skew results.\n",
    "\n",
    "MSE:\n",
    "\n",
    "Advantages: Penalizes larger errors more heavily, which can be useful for capturing the variance.\n",
    "Disadvantages: Also sensitive to outliers and scale, and it is not in the same units as the response variable.\n",
    "\n",
    "MAE:\n",
    "\n",
    "Advantages: Provides a straightforward measure of prediction accuracy in the same units as the response variable. Less sensitive to outliers.\n",
    "Disadvantages: Less sensitive to large errors compared to RMSE, which may not be ideal if large errors are of concern."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b29123-4c58-4a2f-baf6-d873dd2d4816",
   "metadata": {},
   "source": [
    "### Q6. Explain the concept of Lasso regularization. How does it differ from Ridge regularization, and when is it more appropriate to use ?\n",
    "##### Answer:-\n",
    "Lasso Regularization (Least Absolute Shrinkage and Selection Operator) adds a penalty proportional to the absolute value of the coefficients to the cost function:\n",
    "\n",
    "Cost¬†Function =Loss¬†Function +  Œª 1‚àëp |bj|\n",
    "\n",
    "Where  Œª is the regularization parameter. Lasso can drive some coefficients to zero, effectively performing feature selection.\n",
    "\n",
    "Ridge Regularization adds a penalty proportional to the square of the coefficients:\n",
    "\n",
    "Cost¬†Function=Loss¬†Function + Œª 1‚àëp |b^2j|\n",
    "\n",
    "Differences:\n",
    "\n",
    "Lasso: Can eliminate irrelevant predictors (feature selection).\n",
    "Ridge: Shrinks coefficients but does not eliminate predictors.\n",
    "\n",
    "Use Lasso when you need feature selection and prefer a sparse model. Use Ridge when you want to handle multicollinearity but retain all predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9981593e-86d4-40f5-8a5f-3a4445b8b0a2",
   "metadata": {},
   "source": [
    "### Q7. How do regularized linear models help to prevent overfitting in machine learning? Provide an example to illustrate.\n",
    "##### Answer:-\n",
    "Regularized Linear Models (e.g., Ridge, Lasso) help to prevent overfitting by adding a penalty to the size of the coefficients. This penalization discourages complex models with large coefficients, which are more likely to overfit the training data.\n",
    "\n",
    "Example:\n",
    "\n",
    "Ridge Regression: If you have a dataset with many features, Ridge regression will shrink the coefficients of less important features, thereby reducing the model complexity and mitigating overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3325c33-aa21-48f4-a663-32ebf38b02b9",
   "metadata": {},
   "source": [
    "### Q8. Discuss the limitations of regularized linear models and explain why they may not always be the best choice for regression analysis.\n",
    "##### Answer:-\n",
    "Assumption of Linearity: Regularized models assume a linear relationship between predictors and response, which may not always be appropriate.\n",
    "\n",
    "Choice of Regularization Parameter: The performance heavily depends on the choice of Œª, which requires careful tuning.\n",
    "\n",
    "Model Complexity: Regularized models may not capture complex patterns well if the true relationship is non-linear.\n",
    "\n",
    "Interpretability: Lasso's feature selection can make the model less interpretable if too many features are removed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9234d669-10ba-47ed-b2f0-afd3bcd8d84e",
   "metadata": {},
   "source": [
    "### Q9. You are comparing the performance of two regression models using different evaluation metrics. Model A has an RMSE of 10, while Model B has an MAE of 8. Which model would you choose as the better performer, and why? Are there any limitations to your choice of metric?\n",
    "\n",
    "##### Answer:-\n",
    "Choice: Model B would generally be preferred if you are concerned with the average magnitude of errors and less sensitive to large errors, as MAE is less sensitive to outliers compared to RMSE.\n",
    "\n",
    "Limitations:\n",
    "\n",
    "RMSE: If large errors are a significant concern, RMSE might be more informative.\n",
    "MAE: If you need to capture larger errors more explicitly, RMSE might be a better choice.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a522837-9d9e-46eb-8127-d1481e3433b4",
   "metadata": {},
   "source": [
    "### Q10. You are comparing the performance of two regularized linear models using different types of regularization. Model A uses Ridge regularization with a regularization parameter of 0.1, while Model B uses Lasso regularization with a regularization parameter of 0.5. Which model would you choose as thebetter performer, and why? Are there any trade-offs or limitations to your choice of regularization method?\n",
    "### Answer:-\n",
    "Choice:\n",
    "\n",
    "Ridge Regression (Model A): Use if you want to include all features but with reduced coefficients, which is good for multicollinearity but doesn‚Äôt perform feature selection.\n",
    "Lasso Regression (Model B): Use if feature selection is important, and you are okay with some coefficients being exactly zero.\n",
    "\n",
    "Trade-offs:\n",
    "\n",
    "Ridge: Less suitable if feature selection is needed; all features are included.\n",
    "Lasso: May remove too many features, which can result in a loss of important information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8260a71-6223-4935-99f7-4517562e5b71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
